---
title: 'STAT 420: Final Project'
author: "Chenhan Xu (chenhan2)"
date: 'Friday, May 3'
output:
  html_document:
    theme: readable
    toc: yes
---

### Abstract

This report will try to fit a proper linear regression model to predict the `Disease.Severity` in the `disease` dataset using other variables. We will first check the **multicollinearity** of the dataset and try to reduce the predictors. Then we will fit the full **additive** linear models and try to interpret and diagnose them with different methods. **Cook's Distance** will be used to remove the influencial unusual observation. As the model do not fit the data well, **response transformation** is used to find a better one. For each family of models, we use **Bayesian Information Criterion (BIC)** to do the variable selection. BIC is preferred here instead of AIC because with large sample data size, BIC tends to find a smaller model that can be more interpretable. Test of model assumption will be given to each of the models. A comparison is given to different models to find a best one. We will also use **Exhuasitive Search** to identify if the search miss the best model. Finally, this report will arrive at a proper model for the dataset.

### Setup of the project

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")
```

Import the `disease` dataset and the packages that contain the necessary functions for this project. 

```{r, warning = FALSE, message = FALSE}
library(faraway)
library(lmtest)
library(MASS)
library(leaps)
```

```{r}
disease = read.csv("Disease Data.csv")
```

Define some of the functions that we may need for convenience.

```{r}
fitted_vs_resid = function(model) {
  plot(fitted(model), resid(model),
       col = "grey", pch = 20,
       xlab = "Fitted Value",
       ylab = "Residual",
       main = "Fitted Value versus Residual")
  abline(h = 0, lwd = 3, col = "dodgerblue")
}
qqplot = function(model) {
  qqnorm(resid(model), col = "grey", lwd = 3)
  qqline(resid(model), col = "dodgerblue", cex = 1.5, pch = 20)
}
calc_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}
```

### Summary Statistics and Data Visualization

First, look at the variales of the data frame.

```{r}
str(disease)
```

`X` variable is the index of observations. In this case, the data of different patients is independent with the index, so remove `X` from the data frame.

Besides, there is no factor variable in the dataset, so the model will not have dummy variables.

```{r}
disease = subset(disease, select = - X)
```

```{r}
hist(disease$Disease.Severity, breaks = 50,
     border = "dodgerblue")
```

The histogram shows the distribution of `Disease.Severity` is right skewed.

```{r}
boxplot(disease$Disease.Severity, main = "Boxplot of Disease Severity")
```

The boxplot shows that the median of `Disease.Severity` is slightly over 2, but there are a lot of outliers in the dataset.

The table below shows some important statistics of `Disease.Severity`.

```{r}
data_analysis = data.frame(
  "Measure" = c("Mean", "Median", "Variance", "Standard Deviation", "Max", "Min"),
  "Result" = c(mean(disease$Disease.Severity), median(disease$Disease.Severity), var(disease$Disease.Severity), sd(disease$Disease.Severity), max(disease$Disease.Severity), min(disease$Disease.Severity)))
knitr::kable(data_analysis)
```

Before starting fitting the model, check if there is collinearity in between variables to see if we can reduce some variables.

Test the mulitcollinearity using **variance inflation factor (VIF)**.

```{r}
vif(disease)[which(vif(disease) > 5)]
```

Notice that predictors `F2R` and `PLEKHM1` have **vif** greater than 5. Therefore, look at variable added plots and partial correlation coefficients for these two predictors.

First, focus on `F2R`.

```{r}
fit_full_without_F2R = lm(Disease.Severity ~ . - F2R, data = disease)
fit_F2R = lm(F2R ~ . - Disease.Severity, data = disease)
```
```{r}
cor(resid(fit_full_without_F2R), resid(fit_F2R))
```
```{r}
plot(resid(fit_full_without_F2R) ~ resid(fit_F2R),
     col = "dodgerblue", pch = 20,
     xlab = "Residuals, Added Predictor", 
     ylab = "Residuals, Original Model")
abline(h = 0, lty = 2)
abline(v = 0, lty = 2)
abline(lm(resid(fit_full_without_F2R) ~ resid(fit_F2R)),
       col = "darkorange", lwd = 2)
```

Next, look at `PLEKHM1`.

```{r}
fit_full_without_PLEKHM1 = lm(Disease.Severity ~ . - PLEKHM1, data = disease)
fit_PLEKHM1 = lm(PLEKHM1 ~ . - Disease.Severity, data = disease)
```
```{r}
cor(resid(fit_full_without_PLEKHM1), resid(fit_PLEKHM1))
```
```{r}
plot(resid(fit_full_without_PLEKHM1) ~ resid(fit_PLEKHM1),
     col = "dodgerblue", pch = 20,
     xlab = "Residuals, Added Predictor", 
     ylab = "Residuals, Original Model")
abline(h = 0, lty = 2)
abline(v = 0, lty = 2)
abline(lm(resid(fit_full_without_PLEKHM1) ~ resid(fit_PLEKHM1)),
       col = "darkorange", lwd = 2)
```

The partial correlation coefficients of both predictors are unignorable. The variable added plots also show that the variations of `F2R` and `PLEKHM1` have correlation with other predictors. This means that their variations are unexplained by other predictors. Hence, we prefer to keep these two predictors in the model.

### Full Model

Try to fit the full additive model.

```{r}
fit_full_add = lm(Disease.Severity ~ ., data = disease)
```

```{r}
summary(fit_full_add)
```

The full additive model contains 21 coefficients. The F-test shows that the model is significant. However, the single variable t-tests show that many of the predictors in the full additive model are insignificant. 

Next, check if the model violate some of the assumptions.

```{r}
fitted_vs_resid(fit_full_add)
qqplot(fit_full_add)
```

Notice that there are some unusual data points in the plot, so use the **Cook's Distance** to identify and remove them.

```{r}
n = nrow(disease)
p = length(coef(fit_full_add))
```

```{r}
cook_full_add = cooks.distance(fit_full_add)
outlier = which(cook_full_add > 4 / n)
```

Because all of our model is based on the full linear model, drop the unusual influential observations based on the full linear model.

```{r}
disease = disease[-outlier,]
```

```{r}
fit_full_add = lm(Disease.Severity ~ ., data = disease)
```

Now, removed the unusual observations and look back intto the two plots. 

```{r}
fitted_vs_resid(fit_full_add)
qqplot(fit_full_add)
```

Through the plots, it is clearly noticed that the **Linear Assumption**, **Constant Variance Assumption** and **Normality Assumptions** are all violated. In this way, this model is terrible.

Use **BIC** to try to find a better smaller additive model.

```{r}
fit_bic_add = step(fit_full_add, direction = "backward", trace = 0, k = log(n))
``` 

```{r}
fitted_vs_resid(fit_bic_add)
qqplot(fit_bic_add)
```

However, the plots tells that the smaller additive model is again terrible. This matches our expectation since we know that simply removing predictors may not help dealing with the **linear**, **constant variance** and **normality** assumptions.

In the next few sections, we will perform response transformation to find a better model.

### Log mode

The Fitted Value vs Residual plot shown above suggests that a **log-transformation** on response may be helpful.

```{r}
fit_full_log_add = lm(log(Disease.Severity) ~ ., data = disease)
```

Fit a full log model and then use **BIC** to find a proper smaller model.

```{r}
fit_bic_log_add = step(fit_full_log_add, direction = "backward", k = log(n), trace = 0)
```

```{r, echo = FALSE, eval = FALSE}
cook_bic_log = cooks.distance(fit_bic_log_add)
outlier_log_add = which(cook_bic_log > 4 / n)
cook_bic_log[outlier_full_add]
```


```{r, echo = FALSE, eval = FALSE}
fit_full_log_add = lm(log(Disease.Severity) ~ ., data = disease[-outlier_log_add, ])
```

```{r, echo = FALSE, eval = FALSE}
fit_bic_log_add = step(fit_full_log_add, direction = "backward", trace = 0, k = log(n))
```

```{r}
summary(fit_bic_log_add)
```

The backward search suggests a model with 10 predictors as above. Single variable t-tests show that all the predictors are significant. The F-test shows that the model is significant.

Now, look at the model assumptions.

```{r}
fitted_vs_resid(fit_bic_log_add)
qqplot(fit_bic_log_add)
```

```{r}
bptest(fit_bic_log_add)
```

```{r}
shapiro.test(resid(fit_bic_log_add))
```

The Fitted Value vs Residual plot and Q-Q plot look good. Furthermore, the **Breusch-Pagan Test** and **Shapiro-Wilk Test** both have a large p-value. Therefore, the model assumptions are not violated. This model is a good one.

In the following section, we will use **Boxcox Method** to find if there is a better response transformation.

### Boxcox model


```{r}
boxcox(fit_full_add, plotit = TRUE, lambda = seq(-0.5, 0, by = 0.05))
```

The Boxcox plot suggests that $\lambda \in [-0.4,-0.1]$ can be a good $\lambda$ for transformation. Choose $\lambda = -0.25$ as it has relative high log-Likelihood.

Therefore, the transformation of the response is $y^{-0.25} - 1 \over -0.25$.

Fit a linear model based on this response transformation and use **BIC** to find a proper smaller model.

```{r}
fit_boxcox = lm(((Disease.Severity^(-0.25) - 1) / (-0.25)) ~ ., data = disease)
```

```{r}
fit_bic_boxcox = step(fit_boxcox, direction = "backward", trace = 0, k = log(n))
```

```{r, echo = FALSE, eval = FALSE}
cook_boxcox = cooks.distance(fit_bic_boxcox)
outlier_boxcox = which(cook_boxcox > 4 / n)
```

```{r, echo = FALSE, eval = FALSE}
fit_boxcox = lm(((Disease.Severity^(-0.25) - 1) / (-0.25)) ~ ., data = disease[-outlier_boxcox,])
```

```{r, echo = FALSE, eval = FALSE}
fit_bic_boxcox = step(fit_boxcox, direction = "both", trace = 0, k = log(n))
```

Summary the new model.

```{r}
summary(fit_bic_boxcox)
```

The new model has 10 predictors all of which are significant in the model. The $R^2$ is 0.692 in the model. The statistics of the model look pretty good.

Then do the test on the model assumptions.

```{r}
fitted_vs_resid(fit_bic_boxcox)
qqplot(fit_bic_boxcox)
```

```{r}
bptest(fit_bic_boxcox)
shapiro.test(resid(fit_bic_boxcox))
```

The Fitted Value vs Residual plot looks good, the **linear assumption** and **constant variance assumption** are not violated. The Q-Q plot also looks good, the data points fall along the normal line. **Normality assumption** is also not violated.

Furthermore, the Breusch-Pagan test and Shapiro-Wilk test both have large p-value. None of the assumptions is violated.

### Model Comparison

The **Log** model and **Boxcox** model both looks good. Do some comparison to choose a better one.

```{r}
all.equal(rownames(coef(fit_bic_log_add)), rownames(coef(fit_bic_boxcox)))
```

The predictors in both model are exactly the same.

```{r}
summary(fit_bic_log_add)$r.squared
summary(fit_bic_boxcox)$r.squared
```

**Log** model has a comparatively larger $R^2$ value, but the different is insignificant between the models. Therefore, Look at other statistics.

The **Boxcox** model have larger p-values in both **Breusch-Pagan test** and **Shaprio-Wilk test**. In this case, the **Boxcox** model may be slightly better.

Do the cross validation using $RMSE_{LOOCV}$.

```{r}
calc_loocv_rmse(fit_bic_log_add)
calc_loocv_rmse(fit_bic_boxcox)
```

**Boxcox** model also has a smaller $RMSE_{LOOCV}$. Hence, the **Boxcox** model is preferred.

This is within expection. The **Log** transformation is actually part of **Boxcox** method. When $\lambda$ of **Boxcox** equals to $0$, the *Boxcox* transformation goes into a **log** form. In the previous **Boxcox** plot, we notice that $0$ is not in 95% interval, so **log** may not be a best transformation.

### Exhaustive Search

Because the **backward step search** may miss the best model due to its algorithm, we want to use a **exhaustive search** to verity if there is a better model.

```{r}
all_mod = summary(regsubsets(((Disease.Severity^(-0.25) - 1) / (-0.25)) ~ ., data = disease, nvmax = 20))
```

Calculate the **BIC** of all models found by function `regsubsets`.

```{r}
exh_bic = n * log(all_mod$rss / n) + log(n) * (2:p)
```

```{r}
plot(exh_bic ~ I(2:p), ylab = "BIC", xlab = "p, number of parameters", 
     pch = 20, col = "dodgerblue", type = "b", cex = 2,
     main = "BIC vs Model Complexity")
```

The plot above shows the **BIC** of models with different number of parameters found by `regsubsets`

```{r}
which.min(exh_bic)
all_mod$which[which.min(exh_bic),][all_mod$which[which.min(exh_bic),]]
```

The model found by **exhausitive search** has the same response and predictors as the model found by **backward search**. Therefore, the two different methods lead to exactly the same model.

### Conclusion

The best proper model found in the project is the one uses $Disease.Severity^{-0.25} - 1 \over -0.25$ as response and `PCDH12`,`DLG5`,`SHISA5`,`AF161342`,`CARKD`,`F2R`,`PHKG1`,`PLEKHM1`,`PPAN`,`C14orf143` as predictors. The model has a $R^2$ value `0.6922`.

For further improvement, I would suggest to add interaction or polynomial terms in the predicors. The more terms in the predictors will improve the accuracy of prediction, but as a tradeoff, this will make the model complex and hard to interpret. The choice of variables should be based on the need of the model.